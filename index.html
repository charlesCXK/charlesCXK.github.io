<!DOCTYPE html>

<html lang="en">
<head>
    <!-- Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156699410-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-156699410-1');
    </script>
    
    <meta charset="UTF-8">
    <meta http-equiv="Content-Security-Policy" content="block-all-mixed-content">

    <title>Xiaokang Chen ÈôàÂ∞èÂ∫∑</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="css/style.css">
</head>

<div class="navbar navbar-fixed-top">
    <div class="container">
        <strong class="navbar-brand">Xiaokang Chen / ÈôàÂ∞èÂ∫∑</strong>
        <div class="navbar-collapse collapse">
            <ul class="nav navbar-nav navbar-right">
                <li><a href="#about">About</a></li>
                <li><a href="#publication">Publications</a></li>
                <li><a href="#education">Education</a></li>
                <li><a href="#experiences">Experiences</a></li>
                <li><a href="#honors">Honors</a></li>
                <li><a href="#academic_activity">Activities</a></li>
            </ul>
        </div>
    </div>
</div>

<div class="gray-container" id="about">
    <div class="container">
        <div class="row">
            <div class="col-xs-4">
                <img id="me-img" src="images/cxk.png">

                <div style="margin-top: 10%; font-size: large";><strong>Xiaokang Chen / ÈôàÂ∞èÂ∫∑</strong></div>
                <div style="margin-top: 5%"> Ph.D. Student at Peking University</div>
                <div style="margin-top: 5%">  pkucxk@pku.edu.cn </div>
            </div>
            <div class="col-xs-8">
                <h2>
                    <strong>Xiaokang Chen</strong>
<!--                     <a href="">
                        <img src = "images/icon/cv.png" title = "cv" width ="36" height = "36">
                    </a> -->
                    <a href="https://github.com/charlesCXK" target="_blank">
                        <img src = "images/icon/github_square.png" title = "github" width ="24">
                    </a>
                    <a href="https://www.linkedin.com/in/%E5%B0%8F%E5%BA%B7-%E9%99%88-03a149120/" target="_blank">
                        <img src = "images/icon/linkedin.png" title = "linkedin" width ="24">
                    </a>
                </h2>

                <p>
                <img src="images/icon/education.png" title="education" width="24" height="24">  
                I obtained my Ph.D degree at Peking University (PKU) in 2024, supervised by Professor Gang Zeng. Before that, I received my Bachelor‚Äôs degree at Peking University in July 2019.
                <br/>
                </p>

                <br/>
                <p>
                <img src="images/icon/thought.png" title="thoughts" width="24" height="24"> My research interests are in Computer Vision and Multi-Modal Learning, including Visual Pretraining, Scene Understanding (Detection and Segmentation), and Multi-Modal Large Language Models.
                </p>

                <p>
                    <b> I'm open to collaboration and discussions. Let's connect and explore possibilities together! </b>
                </p> -->
                <br>
                <div>
                    <div id="news"><img src="images/icon/news.png" title="news" width="24" height="24"> News</div>
                    <div class="row work-block">
                        <ul>
                            <li>
                                2024.05 &nbsp; I successfully defended my Ph.D thesis!
                            </li>
                            <li>
                                2024.04 &nbsp; One paper on DETR distillation is accepted by <a href="https://ijcai24.org/" target="_blank">IJCAI 2024</a>.
                            </li>
                            <li>
                                2024.01 &nbsp; One paper on Responsible AI (BiasNeuron) is accepted by <a href="https://iclr.cc/" target="_blank">ICLR 2024</a>.
                            </li>
                            <li>
                                2023.09 &nbsp; Our Multi-Modal Large Language Model <a href="https://arxiv.org/pdf/2305.11175.pdf" target="_blank">VisionLLM</a> and a work on Responsible AI (<a href="https://arxiv.org/pdf/2305.15377.pdf" target="_blank">CodeBias</a>) are accepted by <a href="https://nips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a>.
                            </li>
                            <li>
                                2023.07 &nbsp; Our <a href="https://arxiv.org/pdf/2207.13085.pdf" target="_blank">Group DETR</a> and <a href="https://arxiv.org/pdf/2303.02091.pdf" target="_blank">NeRF2Mesh</a> are accepted by <a href="https://iccv2023.thecvf.com/" target="_blank">ICCV 2023</a>. Code is released.
                            </li>
                            <li>
                                2022.09 &nbsp; Our Compressible-Composable NeRF (<a href="https://arxiv.org/abs/2205.14870" target="_blank">CC-NeRF</a>) is accepted by <a href="https://nips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a>. Code is available <a href="https://github.com/ashawkey/CCNeRF" target="_blank">here</a>.
                            </li>
                            <li>
                                2022.09 &nbsp; I am selected as the representative of freshmen to participate in the <a href="https://news.pku.edu.cn/xwzh/b144cb82e43a46558bb8befbbf487797.htm" target="_blank">symposium</a> held by Peking University.
                            </li>
                            
                            <li>
                                2022.07 &nbsp; One paper on Instance Mesh Reconstruction (<a href="https://arxiv.org/abs/2203.16832" target="_blank">DIMR</a>) is accepted by <a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a>. Code is available <a href="https://github.com/ashawkey/dimr" target="_blank">here</a>.
                            </li>
                            <li>
                                2022.02 &nbsp; Please check our <a href="https://arxiv.org/abs/2202.03026" target="_blank">CAE</a>, a novel MIM approach for self-supervised learning.
                            </li>
                            <li> 
                                2021.12 &nbsp; Joined <a href="https://home.baidu.com/" target="_blank">Baidu</a> as a research intern.
                            </li>  
                            <li> 
                                2021.12 &nbsp; One paper is accepted by <a href="https://aaai.org/Conferences/AAAI-22/" target="_blank">AAAI 2022</a>!
                            </li>   
                            <li> 
                                2021.07 &nbsp; Our <a href="https://arxiv.org/abs/2108.06152" target="_blank">Conditional DETR</a> is accepted by <a href="http://iccv2021.thecvf.com/home" target="_blank">ICCV 2021</a>! Code is available <a href="https://git.io/ConditionalDETR" target="_blank">here</a>.
                            </li>   
                            <li> 
                                2021.07 &nbsp; I am selected as <b>"Top 10 Outstanding Researcher" (Â≠¶ÊúØÂçÅÊù∞)</b>, EECS of Peking University.
                            </li>   
                            <li> 
                                2021.07 &nbsp; One paper is accepted by <a href="https://2021.acmmm.org/" target="_blank">ACM MM 2021</a>!
                            </li>   
                            <li> 
                                2021.06 &nbsp; I have released the code and data for our CVPR 2021 paper <b>CPS</b>. Please check <a href="https://github.com/charlesCXK/TorchSemiSeg" target="_blank">here</a>.
                            </li>   
                            <li> 
                                2021.03 &nbsp; One paper is accepted by <a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR 2021</a>!
                            </li>   
                            <li> 
                                2020.07 &nbsp; One paper is accepted by <a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a>!
                            </li>    
                            <li> 
                                2020.06 &nbsp; Joined <a href="https://www.msra.cn/" target="_blank">MSRA</a> as a research intern.
                            </li>    
                            <li> 
                                2020.05 &nbsp; One paper is accepted by <a href="https://2020.ieeeicip.org/" target="_blank">ICIP 2020</a>!
                            </li>    
                            <li> 
                                2020.03 &nbsp; One paper is accepted by <a href="http://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a>!
                            </li>            
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div id="rows" style="margin-left: 8%">


<div id="publication">
    <div class="container">
        <h2 class="section_title">Representative Works  
            <a href="https://scholar.google.com.hk/citations?view_op=list_works&hl=zh-CN&user=qALe908AAAAJ" target="_blank">
                    (<img src = "images/icon/scholar.jpg" title = "scholar" width ="28"> Google Scholar (~2700 citations))
            </a>
        </h2>

    <!-- Paper CAE -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2022_CAE/arch.png">
        </div>

        <div class="col-xs-8">
            <font color=#e67e22><u>Xiaokang Chen</u></font>, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, Jingdong Wang
            <br>
            <strong>CAE: Context Autoencoder for Self-Supervised Representation Learning</strong>
            <br>
            <em><i>International Journal of Computer Vision (<strong>IJCV</strong>)</i>, 2023</em>
            <br>
            [<a href="https://arxiv.org/abs/2202.03026" target="_blank">Paper</a>] [<a href="https://github.com/lxtGH/CAE" target="_blank">Code</a>] [<a href="https://github.com/Atten4Vis/CAE" target="_blank">Code2</a>] [<a href="https://zhuanlan.zhihu.com/p/531243540" target="_blank">‰∏≠ÊñáËß£ËØª</a>]
            <br>
        </div>
    </div>

    <!-- Paper ICCV 2021 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/ICCV2021_CondDETR/arch.png">
        </div>

        <div class="col-xs-8">
            <font color=#e67e22><u>Xiaokang Chen</u></font>*, Depu Meng*, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun and Jingdong Wang (<font color=#e67e22>*: Equal Contribution</font>)
            <br>
            <strong>Conditional DETR for Fast Training Convergence</strong>
            <br>
            <em><i>International Conference on Computer Vision (<strong>ICCV</strong>)</i>, 2021</em>
            <br>
            [<a href="https://arxiv.org/abs/2108.06152" target="_blank">Paper</a>] [<a href="https://git.io/ConditionalDETR" target="_blank">Code</a>] [<a href="https://zhuanlan.zhihu.com/p/401916664" target="_blank">‰∏≠ÊñáËß£ËØª</a>]
            <br>
        </div>
    </div>

    <!-- Paper CVPR 2021 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/CVPR2021_CPS/arch.png">
        </div>

        <div class="col-xs-8">
            <font color=#e67e22><u>Xiaokang Chen</u></font>, Yuhui Yuan, Gang Zeng and Jingdong Wang
            <br>
            <strong>CPS: Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision</strong>
            <br>
            <em><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</i>, 2021</em>
            <br>
            [<a href="https://arxiv.org/pdf/2106.01226.pdf" target="_blank">Paper</a>] [<a href="https://github.com/charlesCXK/TorchSemiSeg" target="_blank">Code</a>] [<a href="papers/CVPR2021_CPS/00446-poster.pdf" target="_blank">Poster</a>] [<a href="papers/CVPR2021_CPS/CVPR2021_CPS_slides.pptx">Slides</a>]  [<a href="https://www.youtube.com/watch?v=5HKitm0O27w" target="_blank">Video (YouTube)</a>] [<a href="https://zhuanlan.zhihu.com/p/378120529" target="_blank">‰∏≠ÊñáËß£ËØª</a>]
            <br>
        </div>
    </div>

    <!-- Paper VisionLLM -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2023_VisionLLM/arch.png">
        </div>

        <div class="col-xs-8">
            Wenhai Wang*, Zhe Chen*, <font color=#e67e22><u>Xiaokang Chen*</u></font>, Jiannan Wu*, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao and Jifeng Dai <br> (<font color=#e67e22>*: Equal Contribution</font>)
            <br>
            <strong>VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks</strong>
            <br>
            <em><i>Neural Information Processing Systems (<strong>NeurIPS</strong>)</i>, 2023</em>
            <br>
            [<a href="https://arxiv.org/pdf/2305.11175.pdf" target="_blank">Paper</a>] [<a href="https://github.com/OpenGVLab/VisionLLM" target="_blank">Code</a>] [<a href="https://github.com/OpenGVLab/InternGPT" target="_blank">Demo</a>]
            <br>
        </div>
    </div>


        <h2 class="section_title">Other Publications  
        </h2>

        <h2 class="section_title">2024
        </h2>

    <!-- Paper D3ETR -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/IJCAI2024_D3ETR/arch.png">
        </div>

        <div class="col-xs-8">
            <font color=#e67e22><u>Xiaokang Chen</u></font>, Jiahui Chen, Yan Liu, Jiaxiang Tang and Gang Zeng <!-- <br> (<font color=#e67e22>*: Xiaokang is the project leader</font>) -->
            <br>
            <strong>D3ETR: Decoder Distillation for Detection Transformer</strong>
            <br>
            <em><i>International Joint Conference on Artificial  Intelligence (<strong>IJCAI</strong>)</i>, 2024</em>
            <br>
            [<a href="https://arxiv.org/abs/2211.09768" target="_blank">Paper</a>]
            <br>
        </div>
    </div>

    <!-- Paper LGM -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2024_LGM/arch.png">
        </div>

        <div class="col-xs-8">
            Jiaxiang Tang, Zhaoxi Chen, <font color=#e67e22><u>Xiaokang Chen</u></font>, Tengfei Wang, Gang Zeng and Ziwei Liu
            <br>
            <strong>LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation</strong>
            <br>
            <em><i>Arxiv Preprint</i>, 2024</em>
            <br>
            [<a href="https://arxiv.org/pdf/2402.05054.pdf" target="_blank">Paper</a>] [<a href="https://github.com/3DTopia/LGM" target="_blank">Code</a>] [<a href="https://me.kiui.moe/lgm/" target="_blank">Project Page</a>]
            <br>
        </div>
    </div>

    <!-- Paper BiasNeuron -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2024_BiasNeuron/arch.png">
        </div>

        <div class="col-xs-8">
            Yan Liu, Yu Liu, <font color=#e67e22><u>Xiaokang Chen</u></font>, Pin-Yu Chen, Daoguang Zan, Min-Yen Kan and Tsung-Yi Ho <br>
            <strong>The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models</strong>
            <br>
            <em><i>International Conference on Learning Representations (<strong>ICLR</strong>)</i>, 2024</em>
            <br>
            <!-- [<a href="https://arxiv.org/pdf/2402.05054.pdf" target="_blank">Paper</a>] -->
            <br>
        </div>
    </div>

    <!-- Paper ICASSP 2024 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2024_GIST/arch.png">
        </div>

        <div class="col-xs-8">
            Yan Liu, Yazheng Yang, <font color=#e67e22><u>Xiaokang Chen</u></font>
            <br>
            <strong>Improving Long Text Understanding with Knowledge Distilled from Summarization Model</strong>
            <br>
            <em><i>IEEE International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>)</i>, 2024</em>
            <br>
            [<a href="" target="_blank">Paper</a>]
            <br>
        </div>
    </div>


        <h2 class="section_title">2023  
        </h2>

    <!-- Paper Group DETR -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2022_GroupDETR/arch.png">
        </div>

        <div class="col-xs-8">
            Qiang Chen*, <font color=#e67e22><u>Xiaokang Chen</u></font>*, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Gang Zeng and Jingdong Wang (<font color=#e67e22>*: Equal Contribution</font>)
            <br>
            <strong>Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment</strong>
            <br>
            <em><i>International Conference on Computer Vision (<strong>ICCV</strong>)</i>, 2023</em>
            <br>
            [<a href="https://arxiv.org/pdf/2207.13085.pdf" target="_blank">Paper</a>] [<a href="https://zhuanlan.zhihu.com/p/549573717" target="_blank">‰∏≠ÊñáËß£ËØª</a>]
            <br>
        </div>
    </div>

    <!-- Paper Code Bias -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2023_CodeBias/arch.png">
        </div>

        <div class="col-xs-8">
            Yan Liu, <font color=#e67e22><u>Xiaokang Chen</u></font> üíå, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan, Jian-Guang LOU, Pin-Yu Chen, Tsung-Ti Ho (<font color=#e67e22>üíå: Corresponding author</font>)
            <br>
            <strong>Uncovering and Quantifying Social Biases in Code Generation</strong>
            <br>
            <em><i>Neural Information Processing Systems (<strong>NeurIPS</strong>)</i>, 2023</em>
            <br>
            [<a href="https://arxiv.org/pdf/2305.15377.pdf" target="_blank">Paper</a>]
            <br>
        </div>
    </div>

    <!-- Paper ICASSP 2023 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/ICASSP2023_CNAT/arch.png">
        </div>

        <div class="col-xs-8">
            Yan Liu, <font color=#e67e22><u>Xiaokang Chen</u></font>, and Qi Dai
            <br>
            <strong>Parallel Sentence-Level Explanation Generation For Real-World Low-Resource Scenarios</strong>
            <br>
            <em><i>IEEE International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>)</i>, 2023</em>
            <br>
            [<a href="https://arxiv.org/pdf/2302.10707.pdf" target="_blank">Paper</a>]
            <br>
        </div>
    </div>

    <!-- Paper ICCV 2023 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/ICCV2023_NeRF2Mesh/arch.png">
        </div>

        <div class="col-xs-8">
            Jiaxiang Tang, Hang Zhou, <font color=#e67e22><u>Xiaokang Chen</u></font>, Tianshu Hu, Errui Ding, Jingdong Wang and Gang Zeng
            <br>
            <strong>Delicate Textured Mesh Recovery from NeRF via Adaptive Surface Refinement</strong>
            <br>
            <em><i>International Conference on Computer Vision (<strong>ICCV</strong>)</i>, 2023</em>
            <br>
            [<a href="https://arxiv.org/pdf/2303.02091.pdf" target="_blank">Paper</a>] [<a href="https://github.com/ashawkey/nerf2mesh" target="_blank">Code</a>]
            <br>
        </div>
    </div>

    <!-- Paper ACL 2023 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/ACL2023_SocialBias/arch.png">
        </div>

        <div class="col-xs-8">
            Yan Liu, Yan Gao, Zhe Su, <font color=#e67e22><u>Xiaokang Chen</u></font>, Elliott Ash and Jian-Guang LOU
            <br>
            <strong>Uncovering and Categorizing Social Biases in Text-to-SQL</strong>
            <br>
            <em><i>Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>)</i>, 2023</em>
            <br>
            [<a href="https://arxiv.org/pdf/2305.16253.pdf" target="_blank">Paper</a>]
            <br>
        </div>
    </div>

    <!-- Paper TMLR 2023 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/TMLR2023_understand/arch.png">
        </div>

        <div class="col-xs-8">
            Jie Zhu*, Jiyang Qi*, Mingyu Ding*, <font color=#e67e22><u>Xiaokang Chen</u></font>, Ping Luo, Xinggang Wang, Wenyu Liu, Leye Wang and Jingdong Wang
            <br>
            <strong>Understanding Self-Supervised Pretraining with Part-Aware Representation
Learning</strong>
            <br>
            <em><i>Transactions on Machine Learning Research (<strong>TMLR</strong>)</i>, 2023</em>
            <br>
            [<a href="https://arxiv.org/pdf/2301.11915.pdf" target="_blank">Paper</a>] [<a href="https://github.com/JiePKU/understand-ssl-part-aware" target="_blank">Code</a>]
            <br>
        </div>
    </div>

    <!-- Paper CAEv2 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2022_CAEv2/arch.png">
        </div>

        <div class="col-xs-8">
            Xinyu Zhang, Jiahui Chen, Junkun Yuan, Qiang Chen, Jian Wang, Xiaodi Wang, Shumin Han, <font color=#e67e22><u>Xiaokang Chen</u></font>, Jimin Pi, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang
            <br>
            <strong>CAE v2: Context Autoencoder with CLIP Target</strong>
            <br>
            <em><i>Transactions on Machine Learning Research (<strong>TMLR</strong>)</i>, 2023</em>
            <br>
            [<a href="https://arxiv.org/abs/2211.09799" target="_blank">Paper</a>]
            <br>
        </div>
    </div>

        <h2 class="section_title">2022 
        </h2>
    <!-- Paper AAAI 2022 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/AAAI2022_SSC/arch.png">
        </div>

        <div class="col-xs-8">
            <font color=#e67e22><u>Xiaokang Chen</u></font>, Jiaxiang Tang, Jingbo Wang and Gang Zeng 
            <!-- <br> -->
            <!-- (<font color=#e67e22>*: Equal Contribution, Xiaokang is the project leader</font>) -->
            <br>
            <strong>Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel Perspective</strong>
            <br>
            <em><i>AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</i>, 2022</em>
            <br>
            [<a href="https://arxiv.org/abs/2112.12925" target="_blank">Paper</a>]
            <br>
        </div>
    </div>

    <!-- Paper CC-Nerf, NeurIPS 2022 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2022_CCNerf/arch.png">
        </div>

        <div class="col-xs-8">
            Jiaxiang Tang, <font color=#e67e22><u>Xiaokang Chen</u></font>, Jingbo Wang and Gang Zeng
            <br>
            <strong>Compressible-composable NeRF via Rank-residual Decomposition</strong>
            <br>
            <em><i>Neural Information Processing Systems (<strong>NeurIPS</strong>)</i>, 2022</em>
            <br>
            [<a href="https://arxiv.org/abs/2205.14870" target="_blank">Paper</a>] [<a href="https://github.com/ashawkey/CCNeRF" target="_blank">Code</a>]
            <br>
        </div>
    </div>

    <!-- Paper DIMR, ECCV 2022 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2022_DIMR/arch.png">
        </div>

        <div class="col-xs-8">
            Jiaxiang Tang, <font color=#e67e22><u>Xiaokang Chen</u></font>, Jingbo Wang and Gang Zeng
            <br>
            <strong>Point Scene Understanding via Disentangled Instance Mesh Reconstruction</strong>
            <br>
            <em><i>European Conference on Computer Vision (<strong>ECCV</strong>)</i>, 2022</em>
            <br>
            [<a href="https://arxiv.org/abs/2203.16832" target="_blank">Paper</a>]
            <br>
        </div>
    </div>



    <!-- Paper ICME 2022 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/ICME2022_MaskGroup/arch.png">
        </div>

        <div class="col-xs-8">
            Min Zhong, Xinghao Chen, <font color=#e67e22><u>Xiaokang Chen</u></font>, Gang Zeng, Yunhe Wang
            <br>
            <strong>MaskGroup: Hierarchical Point Grouping and Masking for 3D Instance Segmentation</strong>
            <br>
            <em><i>IEEE International Conference on Multimedia and Expo (<strong>ICME</strong>)</i>, 2022</em>
            <br>
            [<a href="https://arxiv.org/abs/2203.14662" target="_blank">Paper</a>]
            <br>
        </div>
    </div>


        <h2 class="section_title">2021 
        </h2>

    <!-- Paper ACM MM 2021 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/MM2021_JIIF/arch.png">
        </div>

        <div class="col-xs-8">
            Jiaxiang Tang, <font color=#e67e22><u>Xiaokang Chen</u></font> and Gang Zeng
            <br>
            <strong>Joint Implicit Image Function for Guided Depth Super-Resolution</strong>
            <br>
            <em><i>ACM Multimedia (<strong>ACM MM</strong>)</i>, 2021</em>
            <br>
            [<a href="https://arxiv.org/abs/2107.08717" target="_blank">Paper</a>] [<a href="https://github.com/ashawkey/jiif" target="_blank">Code</a>]
            <br>
        </div>
    </div>

        <h2 class="section_title">2020
        </h2>
    <!-- Paper ECCV 2020 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/ECCV2020_RGBD_Seg/arch.png">
        </div>

        <div class="col-xs-8">
            <font color=#e67e22><u>Xiaokang Chen</u></font>, Kwan-Yee Lin, Jingbo Wang, Wayne Wu, Chen Qian, Hongsheng Li, and Gang Zeng
            <br>
            <strong>Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation</strong>
            <br>
            <em><i>European Conference on Computer Vision (<strong>ECCV</strong>)</i>, 2020</em>
            <br>
            [<a href="https://arxiv.org/abs/2007.09183" target="_blank">Paper</a>] [<a href="https://github.com/charlesCXK/RGBD_Semantic_Segmentation_PyTorch" target="_blank">Code</a>] 
            <br>
        </div>
    </div>

    <!-- Paper CVPR 2020 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/CVPR2020_3D_SketchAware_SSC/arch.png">
        </div>

        <div class="col-xs-8">
            <font color=#e67e22><u>Xiaokang Chen</u></font>, Kwan-Yee Lin, Chen Qian, Gang Zeng and Hongsheng Li
            <br>
            <strong>3D Sketch-aware Semantic Scene Completion via Semi-supervised Structure Prior</strong>
            <br>
            <em><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</i>, 2020</em>
            <br>
            [<a href="https://arxiv.org/abs/2003.14052" target="_blank">Paper</a>] [<a href="https://github.com/charlesCXK/TorchSSC" target="_blank">Code</a>] [<a href="papers/CVPR2020_3D_SketchAware_SSC/02245-supp.pdf" target="_blank">Supplementary Material</a>] [<a href="papers/CVPR2020_3D_SketchAware_SSC/02245-demo.mp4" target="_blank">Demo Video</a>]
            <br>
        </div>
    </div>

    <!-- Paper ICIP 2020 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/ICIP2020_ssc/arch.png">
        </div>

        <div class="col-xs-8">
            <font color=#e67e22><u>Xiaokang Chen</u></font>, Yajie Xing and Gang Zeng
            <br>
            <strong>Real-time Semantic Scene Completion Via Feature Aggregation and Conditioned Prediction</strong>
            <br>
            <em><i>International Conference on Image Processing (<strong>ICIP</strong>)</i>, 2020</em>
            <br>
            [<a href="https://arxiv.org/abs/2303.10967" target="_blank">Paper</a>]
        </div>
    </div>


        <h2 class="section_title">2019
        </h2>

    <!-- Paper ICIP 2019 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/ICIP2019_2.5D_Conv/arch.png">
        </div>

        <div class="col-xs-8">
            Yajie Xing, Jingbo Wang, <font color=#e67e22><u>Xiaokang Chen</u></font> and Gang Zeng
            <br>
            <strong>2.5D Convolution for RGB-D Semantic Segmentation</strong>
            <br>
            <em><i>International Conference on Image Processing (<strong>ICIP</strong>)</i>, 2019</em>
        </div>
    </div>

    <!-- Paper ICIP 2019 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/ICIP2019_Idempotent_Mapping/arch.png">
        </div>

        <div class="col-xs-8">
            Yajie Xing, Jingbo Wang, <font color=#e67e22><u>Xiaokang Chen</u></font> and Gang Zeng
            <br>
            <strong>Coupling Two-Stream RGB-D Semantic Segmentation Network by Idempotent Mappings</strong>
            <br>
            <em><i>International Conference on Image Processing (<strong>ICIP</strong>)</i>, 2019</em>
        </div>
    </div>

        <h2 class="section_title">Preprints
        </h2>

    <!-- Paper D3ETR -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2023_SAN/arch.png">
        </div>

        <div class="col-xs-8">
            <font color=#e67e22><u>Xiaokang Chen*</u></font>, Jiaxiang Tang*, Diwen Wan, Jingbo Wang and Gang Zeng <br> (<font color=#e67e22>*: Equal Contribution, Xiaokang is the project leader</font>)
            <br>
            <strong>Interactive Segment Anything NeRF with Feature Imitation</strong>
            <br>
            <em><i>Arxiv Preprint</i>, 2023</em>
            <br>
            [<a href="https://arxiv.org/abs/2305.16233" target="_blank">Paper</a>] [<a href="https://me.kiui.moe/san/" target="_blank">Project Page</a>]
            <br>
        </div>
    </div>


    
    <!-- Paper Cond-DETR v2 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2022_CondDETRv2/arch.png">
        </div>

        <div class="col-xs-8">
            <font color=#e67e22><u>Xiaokang Chen</u></font>, Fangyun Wei, Gang Zeng and Jingdong Wang
            <br>
            <strong>Conditional DETR V2: Efficient Detection Transformer with Box Queries</strong>
            <br>
            <em><i>Arxiv Preprint</i>, 2022</em>
            <br>
            [<a href="https://arxiv.org/abs/2207.08914" target="_blank">Paper</a>]
            <br>
        </div>
    </div>
    

    <!-- Paper GroupDETRv2 -->
    <div class="row work-block">
        <div class="project col-xs-3">
            <img class="work-img" src="papers/2022_GroupDETRv2/arch.png">
        </div>

        <div class="col-xs-8">
            Qiang Chen, Jian Wang, Chuchu Han, Shan Zhang, Zexian Li, <font color=#e67e22><u>Xiaokang Chen</u></font>, Jiahui Chen, Xiaodi Wang, Shuming Han, Gang Zhang, Haocheng Feng, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang
            <br>
            <strong>Group DETR v2: Strong Object Detector with Encoder-Decoder Pretraining</strong>
            <br>
            <em><i>Arxiv Preprint</i>, 2022</em> 
            <font color=#e67e22>64.5 mAP on COCO test set!</font>             
            <br>
            [<a href="https://arxiv.org/abs/2211.03594" target="_blank">Paper</a>]
            <br>
        </div>
    </div>

</div>

<div id="education">
    <div class="container">
        <h2 class="section_title">Education</h2>
        <div class="row work-block">
            <ul>
                <li>[2019.09-2024.07] &nbsp; Phd. student at Key Laboratory of Perception (MoE), School of AI, Peking University.
                <li>[2015.09-2019.07] &nbsp; Bachelor of Science at <a href="https://eecs.pku.edu.cn/"" target="_blank">School of EECS</a>, Peking University.
            </ul>
        </div>
    </div>
</div>

<div id="experiences">
    <div class="container">
        <h2 class="section_title">Experiences</h2>
        <div class="row work-block">
            <ul>
                <li>[2024.04-now] &nbsp; Research Intern at <a href="https://www.deepseek.com/" target="_blank">DeepSeek AI (High-Flyer AGI)</a>.
                <li>[2023.12-2024.04] &nbsp; Research Intern at <a href="https://www.msra.cn/" target="_blank">MSRA (Microsoft Research Asia)</a>.
                <li>[2022.12-2023.11] &nbsp; Research Intern at <a href="https://www.shlab.org.cn/" target="_blank">Shanghai Artificial Intelligence Laboratory</a>, directed by Dr. <a href="https://whai362.github.io/" target="_blank">Wenhai Wang</a> and Dr. <a href="https://jifengdai.org/" target="_blank">Jifeng Dai</a>.
                <li>[2021.12-2022.12] &nbsp; Research Intern at <a href="https://home.baidu.com/" target="_blank">Baidu (Artificial Intelligence Group)</a>, directed by Dr. <a href="https://jingdongwang2017.github.io/" target="_blank">Jingdong Wang</a>.
                <li>[2021.09-2021.12] &nbsp; Research Intern at <a href="https://www.msra.cn/" target="_blank">MSRA (Microsoft Research Asia)</a>, directed by Dr. <a href="https://jingdongwang2017.github.io/" target="_blank">Jingdong Wang</a> and <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=en" target="_blank">Fangyun Wei</a>.
                <li>[2020.06-2021.09] &nbsp; Research Intern at <a href="https://www.msra.cn/" target="_blank">MSRA (Microsoft Research Asia)</a>, directed by Dr. <a href="https://jingdongwang2017.github.io/" target="_blank">Jingdong Wang</a>.
                <li>[2019.04-2020.05] &nbsp; Research Intern at <a href="https://www.sensetime.com/en/" target="_blank">SenseTime Research</a>, directed by Dr. <a href="https://kwanyeelin.github.io/" target="_blank">Kwan-Yee Lin</a> and Dr. <a href="https://wywu.github.io/" target="_blank">Wayne (Wenyan) Wu</a>.
                <li>[2021.03-2021.06] &nbsp; TA of <i>"Design and Analysis of Computer Algorithms, 2021"</i> at Peking University.
                <li>[2019.03-2019.06] &nbsp; TA of <i>"Design and Analysis of Computer Algorithms, 2019"</i> at Peking University.
            </ul>
        </div>
    </div>
</div>


<div id="honors">
    <div class="container">
        <h2 class="section_title">Selected Honors</h2>
        <div class="row work-block">
            <ul>
                <li>National Scholarship, (Ministry of Education, People's Republic of China), 2021, 2022, 2023 </li>
                <li>Merit Student of Peking University, PKU, 2020, 2021, 2022, 2023 </li>
                <li>Award for Academic Innovation, PKU, 2021 </li>
                <li>Top 10 Outstanding Researcher (Â≠¶ÊúØÂçÅÊù∞), EECS of Peking University, 2021 </li>
                <li>Huawei Scholarship, PKU, 2021 </li>
                <li>Schlumberger Scholarship, PKU, 2020 </li>
                <li>Award for Excellent Research, PKU, 2019 </li>
                <li>Award for Academic Excellents, PKU, 2018 </li>
            </ul>
        </div>
    </div>
</div>

<div id="invited_talk">
    <div class="container">
        <h2 class="section_title">Invited Talks</h2>
        <div class="row work-block">
            <ul>
                <li><i>Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks</i>. Hosted by Huawei, 2023.11 </li>
                <li><i>Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision</i>. Hosted by Microsoft Research, 2021.05 </li>
            </ul>
        </div>
    </div>
</div>
    
<div id="academic_activity">
    <div class="container">
        <h2 class="section_title">Academic Activities</h2>
        <div class="row work-block">
            <ul>
                <li>Conference reviewer of: CVPR (2022,2023,2024), ECCV (2022,2024), ICCV (2021,2023), NeurIPS (2022,2023,2024), ICML (2022), AAAI (2022,2023)   </li>
                <li>Journal reviewer of: IJCV (2021,2022,2023), TPAMI (2021,2023), TIP (2022), TCSVT (2022), Neurocomputing (2022), CVIU (2022). </li>
            </ul>
        </div>
    </div>
</div>

<div class="container footer">
    <div class="row">
        <div class="text-center">
        Xiaokang Chen @2024 &nbsp;&nbsp;&nbsp;&nbsp; Total Visitors:
        <a href='https://www.counter12.com'><img src='https://www.counter12.com/img-82b92YYyyBDcAY24-26.gif' border='0' alt='conter12'></a><script type='text/javascript' src='https://www.counter12.com/ad.js?id=82b92YYyyBDcAY24'></script>
        </div>
    </div>
</div>



</body>
</html>
